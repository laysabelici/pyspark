{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTZpvVuFgoSSlC+1ABF16d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laysabelici/pyspark/blob/main/dataframes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# INSTALAÇÃO PYSPARK"
      ],
      "metadata": {
        "id": "ya7ZnmgMV_Na"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFIAUNDBCSUR",
        "outputId": "f4b7fffc-eb32-4e2a-b627-e78d83fcdf5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJqRaVozCn_C",
        "outputId": "413b1b2d-5445-481f-d620-3f18448c74dd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 120874 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u382-ga-1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u382-ga-1~22.04.1) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u382-ga-1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u382-ga-1~22.04.1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u382-ga-1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u382-ga-1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5zCUaSiDia2",
        "outputId": "6a607e24-4f15-4f59-a8e2-d9bb5cf471b6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.20.1\" 2023-08-24\n",
            "OpenJDK Runtime Environment (build 11.0.20.1+1-post-Ubuntu-0ubuntu122.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.20.1+1-post-Ubuntu-0ubuntu122.04, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FaCh1sSDmrz",
        "outputId": "5f775e26-4558-4466-e615-a2cb3fc9d04a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=f704624a3a870086c7c57c39b40aea35241c1dd82751c6d019bf85b590698860\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "print(pyspark.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRF0RQu_Dn_w",
        "outputId": "b45e55b3-b74c-4834-b855-0835ab0bb02d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CRIAR DATAFRAME VENDAS"
      ],
      "metadata": {
        "id": "JI1AABisV1dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORTA BIBLIOTECAS NECESSÁRIAS\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit"
      ],
      "metadata": {
        "id": "FKgY3mfBD2F_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#INICIALIZA SESSÃO NO SPARK\n",
        "spark = SparkSession.builder.appName('GerarTabelasVendas').getOrCreate()"
      ],
      "metadata": {
        "id": "SvWufoYLEHAR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CRIE UM DATAFRAME\n",
        "dados_de_vendas = [\n",
        "    (1, '2023-10-17', 'Produto A', 10, 20.0, 100),\n",
        "    (2, '2023-10-18', 'Produto B', 20, 40.0, 200),\n",
        "    (3, '2023-10-19', 'Produto C', 30, 60.0, 300),\n",
        "    (4, '2023-10-20', 'Produto D', 40, 80.0, 400),\n",
        "    (5, '2023-10-21', 'Produto E', 50, 100.0, 500),\n",
        "]"
      ],
      "metadata": {
        "id": "zRXJZgA4EUWM"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DEFINE SCHEMA DATAFRAME (COLUNAS)\n",
        "esquema = ['id', 'data_venda', 'produto', 'quantidade', 'preco', 'matricula']"
      ],
      "metadata": {
        "id": "xIdOtVx1E-rE"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CRIAR DATAFRAME (ARQUIVO)\n",
        "df_vendas = spark.createDataFrame(dados_de_vendas, esquema)"
      ],
      "metadata": {
        "id": "qcXpNpmyFLz5"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ADICIONAR UMA COLUNA (TOTAL_VENDAS)\n",
        "df_vendas = df_vendas.withColumn('total_vendas', df_vendas['quantidade'] * df_vendas['preco'])"
      ],
      "metadata": {
        "id": "uRsBhrv9FVO6"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EXIBA DATAFRAME\n",
        "df_vendas.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFJxcmPKGRU3",
        "outputId": "5415fa16-44b2-448c-e88c-5fc09fdc6393"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+---------+----------+-----+---------+------------+\n",
            "| id|data_venda|  produto|quantidade|preco|matricula|total_vendas|\n",
            "+---+----------+---------+----------+-----+---------+------------+\n",
            "|  1|2023-10-17|Produto A|        10| 20.0|      100|       200.0|\n",
            "|  2|2023-10-18|Produto B|        20| 40.0|      200|       800.0|\n",
            "|  3|2023-10-19|Produto C|        30| 60.0|      300|      1800.0|\n",
            "|  4|2023-10-20|Produto D|        40| 80.0|      400|      3200.0|\n",
            "|  5|2023-10-21|Produto E|        50|100.0|      500|      5000.0|\n",
            "+---+----------+---------+----------+-----+---------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DEFINIR ONDE SALVAR CSV\n",
        "caminho_saida_csv = \"/content/caminho_personalizado/vendas.csv\"\n",
        "\n",
        "# Se você tiver um DataFrame chamado df, salve-o como CSV\n",
        "df_vendas.write.csv(caminho_saida_csv, header=True, mode=\"overwrite\")"
      ],
      "metadata": {
        "id": "5gVN-ywdG1sE"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ENCERRE A SESSÃO DE SPARK\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "Tn1ITECXO2JK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CRIAR DATAFRAME VENDEDORES"
      ],
      "metadata": {
        "id": "Zk72MbeAaFZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORTA BIBLIOTECAS NECESSÁRIAS\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit"
      ],
      "metadata": {
        "id": "756WgVDbaFZg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#INICIALIZA SESSÃO NO SPARK\n",
        "spark = SparkSession.builder.appName('GerarTabelasVendores').getOrCreate()"
      ],
      "metadata": {
        "id": "paab51weaFZg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CRIE UM DATAFRAME\n",
        "dados_de_vendedores = [\n",
        "    (100, 'Pedro'),\n",
        "    (200, 'Julia'),\n",
        "    (300, 'Gustavo'),\n",
        "    (400, 'Ana'),\n",
        "    (500, 'Junior'),\n",
        "]"
      ],
      "metadata": {
        "id": "_yAt2gsJaFZg"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(dados_de_vendedores)"
      ],
      "metadata": {
        "id": "Uaznm_zReAi6",
        "outputId": "1cf92f9f-06ee-4855-80d4-47542cfb576d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DEFINE SCHEMA DATAFRAME (COLUNAS)\n",
        "esquema = ['matricula', 'nome']"
      ],
      "metadata": {
        "id": "oBWSX6jxaFZg"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CRIAR DATAFRAME (ARQUIVO)\n",
        "df_vendores = spark.createDataFrame(dados_de_vendedores, esquema)"
      ],
      "metadata": {
        "id": "daxlLg2gaFZh"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EXIBA DATAFRAME\n",
        "df_vendores.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2359a451-57a2-40ce-d9d4-3236976e615d",
        "id": "4PxakJgJaFZh"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+\n",
            "|matricula|   nome|\n",
            "+---------+-------+\n",
            "|      100|  Pedro|\n",
            "|      200|  Julia|\n",
            "|      300|Gustavo|\n",
            "|      400|    Ana|\n",
            "|      500| Junior|\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DEFINIR ONDE SALVAR CSV\n",
        "caminho_saida_csv = \"/content/caminho_personalizado/vendedores.csv\"\n",
        "\n",
        "# Se você tiver um DataFrame chamado df, salve-o como CSV\n",
        "df_vendores.write.csv(caminho_saida_csv, header=True, mode=\"overwrite\")"
      ],
      "metadata": {
        "id": "BzNH2ByhaFZh"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ENCERRE A SESSÃO DE SPARK\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "RRBCSSyMaFZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ETL"
      ],
      "metadata": {
        "id": "1VARTtMiYRJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "FW8sHr_pYY3F"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark=SparkSession.builder.appName('ETLModel').getOrCreate()"
      ],
      "metadata": {
        "id": "ZNTKar3wYopk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CARREGUE OS DADOS DA TABELA DE VENDAS A PARTIR DE UM ARQUIVO CSV\n",
        "vendas_path = '/content/caminho_personalizado/vendas.csv'\n",
        "df_vendas = spark.read.csv(vendas_path, header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "9LsSn6onYwyx"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EXIBIR PRIMEIROS REGISTROS PARA VERIFICAR DADOS\n",
        "df_vendas.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ejmb5nbZIiF",
        "outputId": "8ff99972-6d7c-48da-c5e3-4be11d421e69"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+---------+----------+-----+---------+------------+\n",
            "| id|data_venda|  produto|quantidade|preco|matricula|total_vendas|\n",
            "+---+----------+---------+----------+-----+---------+------------+\n",
            "|  3|2023-10-19|Produto C|        30| 60.0|      300|      1800.0|\n",
            "|  4|2023-10-20|Produto D|        40| 80.0|      400|      3200.0|\n",
            "|  5|2023-10-21|Produto E|        50|100.0|      500|      5000.0|\n",
            "|  1|2023-10-17|Produto A|        10| 20.0|      100|       200.0|\n",
            "|  2|2023-10-18|Produto B|        20| 40.0|      200|       800.0|\n",
            "+---+----------+---------+----------+-----+---------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CARREGUE OS DADOS DA TABELA DE VENDEDORES A PARTIR DE UM ARQUIVO CSV\n",
        "vendedores_path = '/content/caminho_personalizado/vendedores.csv'\n",
        "df_vendedores = spark.read.csv(vendedores_path, header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "px-FZNgJZO4t"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EXIBIR PRIMEIROS REGISTROS PARA VERIFICAR OS DADOS\n",
        "df_vendedores.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLeLSArSb8eZ",
        "outputId": "680ec6cb-6267-4bd8-eae0-a5a02d3ad46c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+\n",
            "|matricula|   nome|\n",
            "+---------+-------+\n",
            "|      300|Gustavo|\n",
            "|      400|    Ana|\n",
            "|      500| Junior|\n",
            "|      100|  Pedro|\n",
            "|      200|  Julia|\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#COMBINAR DATAFRAMES\n",
        "vendas_por_vendedores = df_vendas.join(df_vendedores, df_vendas['matricula']==df_vendedores['matricula'])"
      ],
      "metadata": {
        "id": "9ffd4r8liwte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "esquema = ['matricula','nome','total_vendas']"
      ],
      "metadata": {
        "id": "0sc9gFuPi18j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CRIAR DATAFRAME (ARQUIVO)\n",
        "df_vendas_por_vendores = spark.createDataFrame(vendas_por_vendedores, esquema)"
      ],
      "metadata": {
        "id": "EmOiUEIbixEC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
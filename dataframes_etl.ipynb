{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNMFtV67kPCrd2Aecb+42cg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laysabelici/pyspark/blob/main/dataframes_etl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# INSTALAÇÃO PYSPARK"
      ],
      "metadata": {
        "id": "ya7ZnmgMV_Na"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFIAUNDBCSUR",
        "outputId": "c690269c-9d00-425d-a392-8c170811ecc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJqRaVozCn_C",
        "outputId": "32b4fae8-6cde-44b4-da1e-be0320c04dbd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 120874 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u382-ga-1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u382-ga-1~22.04.1) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u382-ga-1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u382-ga-1~22.04.1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u382-ga-1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u382-ga-1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5zCUaSiDia2",
        "outputId": "0e237f77-07ae-4678-c8f0-188f43734787"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.20.1\" 2023-08-24\n",
            "OpenJDK Runtime Environment (build 11.0.20.1+1-post-Ubuntu-0ubuntu122.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.20.1+1-post-Ubuntu-0ubuntu122.04, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FaCh1sSDmrz",
        "outputId": "0bb57285-deb8-4e3a-dfac-f0ef6dfda0a0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=c7b1163fb596dd785614773afff4da8ee442d31408e2e927a4c0ffa425126a6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "print(pyspark.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRF0RQu_Dn_w",
        "outputId": "2a769033-76bf-49c4-8a36-1861ea31baaa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CRIAR DATAFRAME VENDAS"
      ],
      "metadata": {
        "id": "JI1AABisV1dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORTA BIBLIOTECAS NECESSÁRIAS\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit"
      ],
      "metadata": {
        "id": "FKgY3mfBD2F_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#INICIALIZA SESSÃO NO SPARK\n",
        "spark = SparkSession.builder.appName('GerarTabelasVendas').getOrCreate()"
      ],
      "metadata": {
        "id": "SvWufoYLEHAR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CRIE UM DATAFRAME\n",
        "dados_de_vendas = [\n",
        "    (1, '2023-10-17', 'Produto A', 10, 20.0, 100),\n",
        "    (2, '2023-10-18', 'Produto B', 20, 40.0, 200),\n",
        "    (3, '2023-10-19', 'Produto C', 30, 60.0, 300),\n",
        "    (4, '2023-10-20', 'Produto D', 40, 80.0, 400),\n",
        "    (5, '2023-10-21', 'Produto E', 50, 100.0, 500),\n",
        "]"
      ],
      "metadata": {
        "id": "zRXJZgA4EUWM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DEFINE SCHEMA DATAFRAME (COLUNAS)\n",
        "esquema = ['id', 'data_venda', 'produto', 'quantidade', 'preco', 'matricula']"
      ],
      "metadata": {
        "id": "xIdOtVx1E-rE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CRIAR DATAFRAME (ARQUIVO)\n",
        "df_vendas = spark.createDataFrame(dados_de_vendas, esquema)"
      ],
      "metadata": {
        "id": "qcXpNpmyFLz5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ADICIONAR UMA COLUNA (TOTAL_VENDAS)\n",
        "df_vendas = df_vendas.withColumn('total_vendas', df_vendas['quantidade'] * df_vendas['preco'])"
      ],
      "metadata": {
        "id": "uRsBhrv9FVO6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EXIBA DATAFRAME\n",
        "df_vendas.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFJxcmPKGRU3",
        "outputId": "631adee2-e675-4474-a39f-f059bdf8dce5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+---------+----------+-----+---------+------------+\n",
            "| id|data_venda|  produto|quantidade|preco|matricula|total_vendas|\n",
            "+---+----------+---------+----------+-----+---------+------------+\n",
            "|  1|2023-10-17|Produto A|        10| 20.0|      100|       200.0|\n",
            "|  2|2023-10-18|Produto B|        20| 40.0|      200|       800.0|\n",
            "|  3|2023-10-19|Produto C|        30| 60.0|      300|      1800.0|\n",
            "|  4|2023-10-20|Produto D|        40| 80.0|      400|      3200.0|\n",
            "|  5|2023-10-21|Produto E|        50|100.0|      500|      5000.0|\n",
            "+---+----------+---------+----------+-----+---------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DEFINIR ONDE SALVAR CSV\n",
        "caminho_saida_csv = \"/content/caminho_personalizado/vendas.csv\"\n",
        "\n",
        "# Se você tiver um DataFrame chamado df, salve-o como CSV\n",
        "df_vendas.write.csv(caminho_saida_csv, header=True, mode=\"overwrite\")"
      ],
      "metadata": {
        "id": "5gVN-ywdG1sE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CRIAR DATAFRAME VENDEDORES"
      ],
      "metadata": {
        "id": "Zk72MbeAaFZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORTA BIBLIOTECAS NECESSÁRIAS\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit"
      ],
      "metadata": {
        "id": "756WgVDbaFZg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#INICIALIZA SESSÃO NO SPARK\n",
        "spark = SparkSession.builder.appName('GerarTabelasVendores').getOrCreate()"
      ],
      "metadata": {
        "id": "paab51weaFZg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CRIE UM DATAFRAME\n",
        "dados_de_vendedores = [\n",
        "    (100, 'Pedro'),\n",
        "    (200, 'Julia'),\n",
        "    (300, 'Gustavo'),\n",
        "    (400, 'Ana'),\n",
        "    (500, 'Junior'),\n",
        "]"
      ],
      "metadata": {
        "id": "_yAt2gsJaFZg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(dados_de_vendedores)"
      ],
      "metadata": {
        "id": "Uaznm_zReAi6",
        "outputId": "3047e706-be1c-43a1-96d5-c86c7ce4c09c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DEFINE SCHEMA DATAFRAME (COLUNAS)\n",
        "esquema = ['matricula', 'nome']"
      ],
      "metadata": {
        "id": "oBWSX6jxaFZg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CRIAR DATAFRAME (ARQUIVO)\n",
        "df_vendores = spark.createDataFrame(dados_de_vendedores, esquema)"
      ],
      "metadata": {
        "id": "daxlLg2gaFZh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EXIBA DATAFRAME\n",
        "df_vendores.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f090dd3-37f2-4b5a-bbd3-0a697b872a63",
        "id": "4PxakJgJaFZh"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+\n",
            "|matricula|   nome|\n",
            "+---------+-------+\n",
            "|      100|  Pedro|\n",
            "|      200|  Julia|\n",
            "|      300|Gustavo|\n",
            "|      400|    Ana|\n",
            "|      500| Junior|\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DEFINIR ONDE SALVAR CSV\n",
        "caminho_saida_csv = \"/content/caminho_personalizado/vendedores.csv\"\n",
        "\n",
        "# Se você tiver um DataFrame chamado df, salve-o como CSV\n",
        "df_vendores.write.csv(caminho_saida_csv, header=True, mode=\"overwrite\")"
      ],
      "metadata": {
        "id": "BzNH2ByhaFZh"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CRIAR DATAFRAME VENDAS POR VENDEDOR"
      ],
      "metadata": {
        "id": "1VARTtMiYRJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType"
      ],
      "metadata": {
        "id": "FW8sHr_pYY3F"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark=SparkSession.builder.appName('ETLModel').getOrCreate()"
      ],
      "metadata": {
        "id": "ZNTKar3wYopk"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CARREGUE OS DADOS DA TABELA DE VENDAS A PARTIR DE UM ARQUIVO CSV\n",
        "vendas_path = '/content/caminho_personalizado/vendas.csv'\n",
        "df_vendas = spark.read.csv(vendas_path, header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "9LsSn6onYwyx"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EXIBIR PRIMEIROS REGISTROS PARA VERIFICAR DADOS\n",
        "df_vendas.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ejmb5nbZIiF",
        "outputId": "caed59ee-3625-463a-b00a-d17140ba57a4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+---------+----------+-----+---------+------------+\n",
            "| id|data_venda|  produto|quantidade|preco|matricula|total_vendas|\n",
            "+---+----------+---------+----------+-----+---------+------------+\n",
            "|  3|2023-10-19|Produto C|        30| 60.0|      300|      1800.0|\n",
            "|  4|2023-10-20|Produto D|        40| 80.0|      400|      3200.0|\n",
            "|  5|2023-10-21|Produto E|        50|100.0|      500|      5000.0|\n",
            "|  1|2023-10-17|Produto A|        10| 20.0|      100|       200.0|\n",
            "|  2|2023-10-18|Produto B|        20| 40.0|      200|       800.0|\n",
            "+---+----------+---------+----------+-----+---------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CARREGUE OS DADOS DA TABELA DE VENDEDORES A PARTIR DE UM ARQUIVO CSV\n",
        "vendedores_path = '/content/caminho_personalizado/vendedores.csv'\n",
        "df_vendedores = spark.read.csv(vendedores_path, header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "px-FZNgJZO4t"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EXIBIR PRIMEIROS REGISTROS PARA VERIFICAR OS DADOS\n",
        "df_vendedores.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLeLSArSb8eZ",
        "outputId": "4f8987c8-6861-4c4f-bea2-ea97407e6ae6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+\n",
            "|matricula|   nome|\n",
            "+---------+-------+\n",
            "|      300|Gustavo|\n",
            "|      400|    Ana|\n",
            "|      500| Junior|\n",
            "|      100|  Pedro|\n",
            "|      200|  Julia|\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#COMBINAÇÃO DE DATAFRAMES PARA CRIAR UM NOVO E SELECIONAR APENAS AS COLUNAS DESEJADAS\n",
        "vendas_por_vendedores = df_vendas.join(df_vendedores, df_vendas['matricula'] == df_vendedores['matricula']).select(df_vendedores['matricula'], 'nome', 'total_vendas')"
      ],
      "metadata": {
        "id": "NHa4dDDPlyiT"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "esquema = ['matricula', 'nome', 'total_vendas']"
      ],
      "metadata": {
        "id": "s6T0cvLNrpXL"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EXIBIR RESULTADO\n",
        "vendas_por_vendedores.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vv1i_5tEmFrC",
        "outputId": "f6d7fb21-fc98-4637-aff5-b4f0b2a3801b"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+------------+\n",
            "|matricula|   nome|total_vendas|\n",
            "+---------+-------+------------+\n",
            "|      300|Gustavo|      1800.0|\n",
            "|      400|    Ana|      3200.0|\n",
            "|      500| Junior|      5000.0|\n",
            "|      100|  Pedro|       200.0|\n",
            "|      200|  Julia|       800.0|\n",
            "+---------+-------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DEFINIR ONDE SALVAR CSV\n",
        "caminho_saida_csv = \"/content/caminho_personalizado/vendas_por_vendedores.csv\"\n",
        "\n",
        "# Se você tiver um DataFrame chamado df, salve-o como CSV\n",
        "vendas_por_vendedores.write.csv(caminho_saida_csv, header=True, mode=\"overwrite\")"
      ],
      "metadata": {
        "id": "2_SZTLTkm7x5"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HIAc8rkuqi3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ETL"
      ],
      "metadata": {
        "id": "y_ObH6yXtUtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Inicialize a sessão Spark\n",
        "spark = SparkSession.builder.appName('ETLModel').getOrCreate()\n",
        "\n",
        "# Carregue os dados da tabela de vendas por vendedor a partir de um arquivo CSV\n",
        "vendas_por_vendedores_path = '/caminho_do_arquivo/vendas_por_vendedores.csv'\n",
        "df_vendas_por_vendedores = spark.read.csv(vendas_por_vendedores_path, header=True, inferSchema=True)\n",
        "\n",
        "# Realize as transformações necessárias\n",
        "# Exemplo de transformação: Calcular a média das vendas por vendedor\n",
        "df_vendas_por_vendedores = df_vendas_por_vendedores.groupBy('matricula', 'nome').agg({'total_vendas': 'avg'})\n",
        "\n",
        "# Defina o caminho para salvar o resultado\n",
        "caminho_saida_csv = \"/caminho_de_saida/vendas_por_vendedores_transformado.csv\"\n",
        "\n",
        "# Salve o DataFrame transformado como CSV\n",
        "df_vendas_por_vendedores.write.csv(caminho_saida_csv, header=True, mode=\"overwrite\")\n",
        "\n",
        "# Encerre a sessão Spark\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "hPxHfgETtXGd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}